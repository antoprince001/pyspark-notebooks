{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19e42a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.5\n",
      "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845498 sha256=2b2db52589fb2997ff77687f25f7c3d63142283a89d8cf7ff167ad2d63979edd\n",
      "  Stored in directory: /Users/antonyprincej/Library/Caches/pip/wheels/9c/aa/b1/8433fd8b1afe7eb31196cc74a42cd778bcb52636a428da079d\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b95e068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0536796",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/26 17:38:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/01/26 17:38:51 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Create SparkSession \n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[1]\") \\\n",
    "      .appName(\"com.spark-rdd\") \\\n",
    "      .getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da4d725f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c124f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RDD from parallelize    \n",
    "dataList = [(\"Java\", 20000), (\"Python\", 100000), (\"Scala\", 3000)]\n",
    "rdd = sc.parallelize(dataList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3983a931",
   "metadata": {},
   "source": [
    "#### Create RDD from external Data source\n",
    "rdd2 = sc.textFile(\"/path/test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba60cd29",
   "metadata": {},
   "source": [
    "On PySpark RDD, you can perform two kinds of operations.\n",
    "\n",
    "RDD transformations – Transformations are lazy operations. When you run a transformation(for example update), instead of updating a current RDD, these operations return another RDD.\n",
    "\n",
    "RDD actions – operations that trigger computation and return RDD values to the driver."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd18b209",
   "metadata": {},
   "source": [
    "### DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2681eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('James','','Smith','1991-04-01','M',3000),\n",
    "  ('Michael','Rose','','2000-05-19','M',4000),\n",
    "  ('Robert','','Williams','1978-09-05','M',4000),\n",
    "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
    "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69e33bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b873c08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 0) / 1]\r",
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd99426",
   "metadata": {},
   "source": [
    "df = spark.read.csv(\"./sample.csv\")\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "DataFrame has a rich set of API which supports reading and writing several file formats\n",
    "\n",
    "csv\n",
    "text\n",
    "Avro\n",
    "Parquet\n",
    "tsv\n",
    "xml and many more"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7bc4ce",
   "metadata": {},
   "source": [
    "## Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da550c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"PERSON_DATA\")\n",
    "df2 = spark.sql(\"SELECT * from PERSON_DATA\")\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aceece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupDF = spark.sql(\"SELECT gender, count(*) from PERSON_DATA group by gender\")\n",
    "groupDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf14bd08",
   "metadata": {},
   "source": [
    "## PySpark Streaming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f74005",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = spark.readStream\n",
    "      .format(\"socket\")\n",
    "      .option(\"host\",\"localhost\")\n",
    "      .option(\"port\",\"9090\")\n",
    "      .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc8f7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = count.writeStream\n",
    "      .format(\"console\")\n",
    "      .outputMode(\"complete\")\n",
    "      .start()\n",
    "      .awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350dd047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka\n",
    "\n",
    "df = spark.readStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", \"192.168.1.100:9092\")\n",
    "        .option(\"subscribe\", \"json_topic\")\n",
    "        .option(\"startingOffsets\", \"earliest\") // From starting\n",
    "        .load()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeae0ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.selectExpr(\"CAST(id AS STRING) AS key\", \"to_json(struct(*)) AS value\")\n",
    "   .writeStream\n",
    "   .format(\"kafka\")\n",
    "   .outputMode(\"append\")\n",
    "   .option(\"kafka.bootstrap.servers\", \"192.168.1.100:9092\")\n",
    "   .option(\"topic\", \"josn_data_topic\")\n",
    "   .start()\n",
    "   .awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1241ca",
   "metadata": {},
   "source": [
    "## Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f19d56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From local[5] : 1\n",
      "parallelize : 6\n"
     ]
    }
   ],
   "source": [
    "# Create spark session with local[5]\n",
    "rdd = spark.sparkContext.parallelize(range(0,20))\n",
    "print(\"From local[5] : \"+str(rdd.getNumPartitions()))\n",
    "\n",
    "# Use parallelize with 6 partitions\n",
    "rdd1 = spark.sparkContext.parallelize(range(0,25), 6)\n",
    "print(\"parallelize : \"+str(rdd1.getNumPartitions()))\n",
    "\n",
    "# rddFromFile = spark.sparkContext.textFile(\"src/main/resources/test.txt\",10)\n",
    "# print(\"TextFile : \"+str(rddFromFile.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf278fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#repartition() method is used to increase or decrease the partitions.\n",
    "# Using repartition\n",
    "rdd2 = rdd1.repartition(4)\n",
    "print(\"Repartition size : \"+str(rdd2.getNumPartitions()))\n",
    "rdd2.saveAsTextFile(\"/tmp/re-partition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38ae59d",
   "metadata": {},
   "source": [
    "RDD coalesce() is used only to reduce the number of partitions. This is an optimized or improved version of repartition() where the movement of the data across the partitions is lower using coalesce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6d4dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using coalesce()\n",
    "rdd3 = rdd1.coalesce(4)\n",
    "print(\"Repartition size : \"+str(rdd3.getNumPartitions()))\n",
    "rdd3.saveAsTextFile(\"/tmp/coalesce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628e107d",
   "metadata": {},
   "source": [
    "## Broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93808a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('James', 'Smith', 'USA', 'California'), ('Michael', 'Rose', 'USA', 'New York'), ('Robert', 'Williams', 'USA', 'California'), ('Maria', 'Jones', 'USA', 'Florida')]\n"
     ]
    }
   ],
   "source": [
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "result = rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3063501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n",
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|James    |Smith   |USA    |CA   |\n",
      "|Michael  |Rose    |USA    |NY   |\n",
      "|Robert   |Williams|USA    |CA   |\n",
      "|Maria    |Jones   |USA    |FL   |\n",
      "+---------+--------+-------+-----+\n",
      "\n",
      "+---------+--------+-------+----------+\n",
      "|firstname|lastname|country|state     |\n",
      "+---------+--------+-------+----------+\n",
      "|James    |Smith   |USA    |California|\n",
      "|Michael  |Rose    |USA    |New York  |\n",
      "|Robert   |Williams|USA    |California|\n",
      "|Maria    |Jones   |USA    |Florida   |\n",
      "+---------+--------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "result = df.rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).toDF(columns)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "722ef4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkContext stop() method\n",
    "spark.sparkContext.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5df1ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
